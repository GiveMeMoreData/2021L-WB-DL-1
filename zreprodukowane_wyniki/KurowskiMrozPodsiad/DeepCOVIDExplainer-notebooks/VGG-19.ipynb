{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('data/x_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "x_test = np.load('data/x_test.npy')\n",
    "y_test = np.load('data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 224, 224, 3)\n",
      "(30, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train /= 255????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import initializers\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential,load_model,Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import *\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras import callbacks\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as sklm\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from utils import lossprettifier\n",
    "from Classifier.VGG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "np.random.seed(3768)\n",
    "\n",
    "# use this environment flag to change which GPU to use \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # specify which GPU(s) to be used\n",
    "\n",
    "#Get TensorFlow session\n",
    "def get_session(): \n",
    "  config = tf.ConfigProto() \n",
    "  config.gpu_options.allow_growth = True \n",
    "  return tf.Session(config=config) \n",
    "  \n",
    "# One hot encoding of labels \n",
    "def dense_to_one_hot(labels_dense,num_clases=4):\n",
    "  return np.eye(num_clases)[labels_dense]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and test sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = dense_to_one_hot(y_train,num_clases=4)\n",
    "y_valid= dense_to_one_hot(y_valid,num_clases=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\envs\\wbvenv36\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:349: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "#Image data generation for the training \n",
    "datagen = ImageDataGenerator(\n",
    "               featurewise_center = False, \n",
    "               samplewise_center = False,  # set each sample mean to 0\n",
    "               featurewise_std_normalization = True,  \n",
    "               samplewise_std_normalization = False)  \n",
    "\n",
    "datagen.fit(x_train) \n",
    "for i in range(len(x_test)):\n",
    "      x_test[i] = datagen.standardize(x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 1.1469 - accuracy: 0.3813 - val_loss: 1.0752 - val_accuracy: 0.5333\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.9644 - accuracy: 0.5103 - val_loss: 1.0205 - val_accuracy: 0.5333\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 3s 92ms/step - loss: 0.7291 - accuracy: 0.7038 - val_loss: 1.1657 - val_accuracy: 0.6333\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 3s 97ms/step - loss: 0.5836 - accuracy: 0.7526 - val_loss: 1.2004 - val_accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 3s 95ms/step - loss: 0.4947 - accuracy: 0.8052 - val_loss: 1.4091 - val_accuracy: 0.6667\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 3s 99ms/step - loss: 0.3958 - accuracy: 0.8382 - val_loss: 1.4711 - val_accuracy: 0.6000\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.3666 - accuracy: 0.8519 - val_loss: 1.7593 - val_accuracy: 0.6667\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.3434 - accuracy: 0.8680 - val_loss: 1.3675 - val_accuracy: 0.7667\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 3s 98ms/step - loss: 0.3317 - accuracy: 0.8918 - val_loss: 1.0582 - val_accuracy: 0.6667\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 3s 94ms/step - loss: 0.2344 - accuracy: 0.9072 - val_loss: 1.9896 - val_accuracy: 0.6333\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.2459 - accuracy: 0.9076 - val_loss: 1.7798 - val_accuracy: 0.6333\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 3s 90ms/step - loss: 0.2410 - accuracy: 0.9044 - val_loss: 1.5429 - val_accuracy: 0.6000\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 3s 95ms/step - loss: 0.2441 - accuracy: 0.9062 - val_loss: 1.8437 - val_accuracy: 0.6000\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.2460 - accuracy: 0.9160 - val_loss: 1.9604 - val_accuracy: 0.6000\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 3s 103ms/step - loss: 0.2445 - accuracy: 0.9227 - val_loss: 2.6526 - val_accuracy: 0.6333\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.2536 - accuracy: 0.9076 - val_loss: 3.0986 - val_accuracy: 0.5333\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 3s 90ms/step - loss: 0.1597 - accuracy: 0.9381 - val_loss: 2.4553 - val_accuracy: 0.6000\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 3s 95ms/step - loss: 0.1256 - accuracy: 0.9464 - val_loss: 2.7855 - val_accuracy: 0.6000\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.2191 - accuracy: 0.9223 - val_loss: 2.7994 - val_accuracy: 0.5333\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 3s 96ms/step - loss: 0.2071 - accuracy: 0.9247 - val_loss: 2.6021 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 3s 92ms/step - loss: 0.1857 - accuracy: 0.9349 - val_loss: 3.4147 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.4887 - accuracy: 0.8021 - val_loss: 0.8778 - val_accuracy: 0.6333\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.2830 - accuracy: 0.8773 - val_loss: 2.0481 - val_accuracy: 0.6000\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 3s 90ms/step - loss: 0.2733 - accuracy: 0.9034 - val_loss: 2.2164 - val_accuracy: 0.6000\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 3s 90ms/step - loss: 0.2157 - accuracy: 0.9202 - val_loss: 3.2277 - val_accuracy: 0.5333\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.1238 - accuracy: 0.9546 - val_loss: 3.5035 - val_accuracy: 0.6000\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.3145 - accuracy: 0.8918 - val_loss: 1.7526 - val_accuracy: 0.6333\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.2534 - accuracy: 0.9072 - val_loss: 3.5187 - val_accuracy: 0.5333\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 3s 94ms/step - loss: 0.2908 - accuracy: 0.8992 - val_loss: 2.2150 - val_accuracy: 0.5333\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.2575 - accuracy: 0.9124 - val_loss: 2.9273 - val_accuracy: 0.6000\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 3s 92ms/step - loss: 0.2366 - accuracy: 0.9304 - val_loss: 2.3276 - val_accuracy: 0.6000\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.2268 - accuracy: 0.9160 - val_loss: 2.2471 - val_accuracy: 0.6333\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 3s 94ms/step - loss: 0.2379 - accuracy: 0.9240 - val_loss: 2.3930 - val_accuracy: 0.5667\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 3s 92ms/step - loss: 0.3499 - accuracy: 0.8732 - val_loss: 2.0294 - val_accuracy: 0.6000\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.2213 - accuracy: 0.9299 - val_loss: 3.6122 - val_accuracy: 0.5333\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 3s 96ms/step - loss: 0.5571 - accuracy: 0.7994 - val_loss: 1.4858 - val_accuracy: 0.4333\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.3641 - accuracy: 0.8557 - val_loss: 2.5177 - val_accuracy: 0.5667\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.2562 - accuracy: 0.9047 - val_loss: 1.8095 - val_accuracy: 0.6000\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 3s 92ms/step - loss: 0.2692 - accuracy: 0.9021 - val_loss: 2.8310 - val_accuracy: 0.4667\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.2474 - accuracy: 0.9076 - val_loss: 2.1316 - val_accuracy: 0.6000\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.2520 - accuracy: 0.9144 - val_loss: 1.5236 - val_accuracy: 0.6000\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 3s 90ms/step - loss: 0.2124 - accuracy: 0.9227 - val_loss: 1.7178 - val_accuracy: 0.7000\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 3s 99ms/step - loss: 0.1867 - accuracy: 0.9286 - val_loss: 2.0413 - val_accuracy: 0.7000\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.1364 - accuracy: 0.9526 - val_loss: 1.3818 - val_accuracy: 0.6667\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 3s 98ms/step - loss: 0.2192 - accuracy: 0.9317 - val_loss: 1.5653 - val_accuracy: 0.6333\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0617 - accuracy: 0.9779 - val_loss: 3.1972 - val_accuracy: 0.6667\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 3s 100ms/step - loss: 0.1404 - accuracy: 0.9485 - val_loss: 2.3542 - val_accuracy: 0.5333\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.1447 - accuracy: 0.9559 - val_loss: 1.9027 - val_accuracy: 0.7000\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 3s 96ms/step - loss: 0.1412 - accuracy: 0.9569 - val_loss: 2.9897 - val_accuracy: 0.6000\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 3s 92ms/step - loss: 0.1951 - accuracy: 0.9403 - val_loss: 1.6668 - val_accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "#Defining hyperparameters\n",
    "batch_Size = 32\n",
    "steps_Per_Epoch = 32\n",
    "numEpochs = 50\n",
    "\n",
    "#Instantating VGG19 model\n",
    "model = VGG19((224,224,3),4) #VGG19_dense for revised VGG19, VGG19 for VGG19. Please pay attention to VGG16(), chnage the input shape and class number in VGG.py.\n",
    "\n",
    "#Creating an optimizers\n",
    "adaDelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.95, nesterov=True)\n",
    "model.compile(optimizer = sgd , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#Creating early stopping \n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 50, verbose = 1, mode = 'auto', restore_best_weights = True)       \n",
    "\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size = batch_Size)\n",
    "validation_generator = datagen.flow(x_valid, y_valid, batch_size = batch_Size)\n",
    "\n",
    "# Model training\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = steps_Per_Epoch,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = 16,\n",
    "    epochs = numEpochs,\n",
    "    shuffle = True, \n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"VGG19_COVID19.h5\"\n",
    "resultPath = 'VGG19_COVID19.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | LossA: 1.15(+0.00%) \u001b[0m\t| LossAB: 1.08(+0.00%) \u001b[0m\t\n",
      "Epoch     1 | LossA: \u001b[32m0.97(-15.88%) ▼\u001b[0m\t| LossAB: \u001b[32m1.02(-5.08%) ▼\u001b[0m\t\n",
      "Epoch     2 | LossA: \u001b[32m0.73(-24.77%) ▼\u001b[0m\t| LossAB: \u001b[91m1.17(+14.23%) ▲\u001b[0m\t\n",
      "Epoch     3 | LossA: \u001b[32m0.59(-18.56%) ▼\u001b[0m\t| LossAB: \u001b[91m1.20(+2.97%) ▲\u001b[0m\t\n",
      "Epoch     4 | LossA: \u001b[32m0.49(-16.64%) ▼\u001b[0m\t| LossAB: \u001b[91m1.41(+17.39%) ▲\u001b[0m\t\n",
      "Epoch     5 | LossA: \u001b[32m0.40(-18.11%) ▼\u001b[0m\t| LossAB: \u001b[91m1.47(+4.40%) ▲\u001b[0m\t\n",
      "Epoch     6 | LossA: \u001b[32m0.37(-7.91%) ▼\u001b[0m\t| LossAB: \u001b[91m1.76(+19.59%) ▲\u001b[0m\t\n",
      "Epoch     7 | LossA: \u001b[32m0.33(-10.20%) ▼\u001b[0m\t| LossAB: \u001b[32m1.37(-22.27%) ▼\u001b[0m\t\n",
      "Epoch     8 | LossA: \u001b[91m0.34(+1.01%) ▲\u001b[0m\t| LossAB: \u001b[32m1.06(-22.62%) ▼\u001b[0m\t\n",
      "Epoch     9 | LossA: \u001b[32m0.24(-30.08%) ▼\u001b[0m\t| LossAB: \u001b[91m1.99(+88.01%) ▲\u001b[0m\t\n",
      "Epoch    10 | LossA: \u001b[91m0.25(+7.05%) ▲\u001b[0m\t| LossAB: \u001b[32m1.78(-10.55%) ▼\u001b[0m\t\n",
      "Epoch    11 | LossA: \u001b[32m0.24(-4.13%) ▼\u001b[0m\t| LossAB: \u001b[32m1.54(-13.31%) ▼\u001b[0m\t\n",
      "Epoch    12 | LossA: \u001b[91m0.25(+4.58%) ▲\u001b[0m\t| LossAB: \u001b[91m1.84(+19.50%) ▲\u001b[0m\t\n",
      "Epoch    13 | LossA: \u001b[32m0.25(-0.67%) ▼\u001b[0m\t| LossAB: \u001b[91m1.96(+6.33%) ▲\u001b[0m\t\n",
      "Epoch    14 | LossA: \u001b[32m0.23(-7.79%) ▼\u001b[0m\t| LossAB: \u001b[91m2.65(+35.31%) ▲\u001b[0m\t\n",
      "Epoch    15 | LossA: \u001b[91m0.25(+9.49%) ▲\u001b[0m\t| LossAB: \u001b[91m3.10(+16.81%) ▲\u001b[0m\t\n",
      "Epoch    16 | LossA: \u001b[32m0.17(-34.94%) ▼\u001b[0m\t| LossAB: \u001b[32m2.46(-20.76%) ▼\u001b[0m\t\n",
      "Epoch    17 | LossA: \u001b[32m0.13(-20.95%) ▼\u001b[0m\t| LossAB: \u001b[91m2.79(+13.45%) ▲\u001b[0m\t\n",
      "Epoch    18 | LossA: \u001b[91m0.22(+68.26%) ▲\u001b[0m\t| LossAB: \u001b[91m2.80(+0.50%) ▲\u001b[0m\t\n",
      "Epoch    19 | LossA: \u001b[32m0.20(-6.72%) ▼\u001b[0m\t| LossAB: \u001b[32m2.60(-7.05%) ▼\u001b[0m\t\n",
      "Epoch    20 | LossA: \u001b[32m0.18(-12.66%) ▼\u001b[0m\t| LossAB: \u001b[91m3.41(+31.23%) ▲\u001b[0m\t\n",
      "Epoch    21 | LossA: \u001b[91m0.49(+173.69%) ▲\u001b[0m\t| LossAB: \u001b[32m0.88(-74.29%) ▼\u001b[0m\t\n",
      "Epoch    22 | LossA: \u001b[32m0.29(-41.68%) ▼\u001b[0m\t| LossAB: \u001b[91m2.05(+133.31%) ▲\u001b[0m\t\n",
      "Epoch    23 | LossA: \u001b[32m0.27(-7.01%) ▼\u001b[0m\t| LossAB: \u001b[91m2.22(+8.22%) ▲\u001b[0m\t\n",
      "Epoch    24 | LossA: \u001b[32m0.22(-18.60%) ▼\u001b[0m\t| LossAB: \u001b[91m3.23(+45.63%) ▲\u001b[0m\t\n",
      "Epoch    25 | LossA: \u001b[32m0.13(-41.44%) ▼\u001b[0m\t| LossAB: \u001b[91m3.50(+8.55%) ▲\u001b[0m\t\n",
      "Epoch    26 | LossA: \u001b[91m0.31(+146.47%) ▲\u001b[0m\t| LossAB: \u001b[32m1.75(-49.97%) ▼\u001b[0m\t\n",
      "Epoch    27 | LossA: \u001b[32m0.25(-18.40%) ▼\u001b[0m\t| LossAB: \u001b[91m3.52(+100.77%) ▲\u001b[0m\t\n",
      "Epoch    28 | LossA: \u001b[91m0.30(+17.30%) ▲\u001b[0m\t| LossAB: \u001b[32m2.22(-37.05%) ▼\u001b[0m\t\n",
      "Epoch    29 | LossA: \u001b[32m0.25(-14.68%) ▼\u001b[0m\t| LossAB: \u001b[91m2.93(+32.16%) ▲\u001b[0m\t\n",
      "Epoch    30 | LossA: \u001b[32m0.23(-11.07%) ▼\u001b[0m\t| LossAB: \u001b[32m2.33(-20.49%) ▼\u001b[0m\t\n",
      "Epoch    31 | LossA: \u001b[91m0.23(+0.41%) ▲\u001b[0m\t| LossAB: \u001b[32m2.25(-3.46%) ▼\u001b[0m\t\n",
      "Epoch    32 | LossA: \u001b[91m0.25(+10.53%) ▲\u001b[0m\t| LossAB: \u001b[91m2.39(+6.49%) ▲\u001b[0m\t\n",
      "Epoch    33 | LossA: \u001b[91m0.34(+37.08%) ▲\u001b[0m\t| LossAB: \u001b[32m2.03(-15.20%) ▼\u001b[0m\t\n",
      "Epoch    34 | LossA: \u001b[32m0.22(-35.83%) ▼\u001b[0m\t| LossAB: \u001b[91m3.61(+78.00%) ▲\u001b[0m\t\n",
      "Epoch    35 | LossA: \u001b[91m0.56(+151.36%) ▲\u001b[0m\t| LossAB: \u001b[32m1.49(-58.87%) ▼\u001b[0m\t\n",
      "Epoch    36 | LossA: \u001b[32m0.36(-35.50%) ▼\u001b[0m\t| LossAB: \u001b[91m2.52(+69.45%) ▲\u001b[0m\t\n",
      "Epoch    37 | LossA: \u001b[32m0.25(-29.72%) ▼\u001b[0m\t| LossAB: \u001b[32m1.81(-28.13%) ▼\u001b[0m\t\n",
      "Epoch    38 | LossA: \u001b[91m0.27(+8.74%) ▲\u001b[0m\t| LossAB: \u001b[91m2.83(+56.45%) ▲\u001b[0m\t\n",
      "Epoch    39 | LossA: \u001b[32m0.25(-8.22%) ▼\u001b[0m\t| LossAB: \u001b[32m2.13(-24.71%) ▼\u001b[0m\t\n",
      "Epoch    40 | LossA: \u001b[91m0.26(+2.52%) ▲\u001b[0m\t| LossAB: \u001b[32m1.52(-28.52%) ▼\u001b[0m\t\n",
      "Epoch    41 | LossA: \u001b[32m0.20(-21.24%) ▼\u001b[0m\t| LossAB: \u001b[91m1.72(+12.74%) ▲\u001b[0m\t\n",
      "Epoch    42 | LossA: \u001b[32m0.19(-6.21%) ▼\u001b[0m\t| LossAB: \u001b[91m2.04(+18.84%) ▲\u001b[0m\t\n",
      "Epoch    43 | LossA: \u001b[32m0.14(-26.63%) ▼\u001b[0m\t| LossAB: \u001b[32m1.38(-32.31%) ▼\u001b[0m\t\n",
      "Epoch    44 | LossA: \u001b[91m0.22(+60.13%) ▲\u001b[0m\t| LossAB: \u001b[91m1.57(+13.28%) ▲\u001b[0m\t\n",
      "Epoch    45 | LossA: \u001b[32m0.07(-70.86%) ▼\u001b[0m\t| LossAB: \u001b[91m3.20(+104.25%) ▲\u001b[0m\t\n",
      "Epoch    46 | LossA: \u001b[91m0.14(+118.05%) ▲\u001b[0m\t| LossAB: \u001b[32m2.35(-26.37%) ▼\u001b[0m\t\n",
      "Epoch    47 | LossA: \u001b[32m0.14(-2.29%) ▼\u001b[0m\t| LossAB: \u001b[32m1.90(-19.18%) ▼\u001b[0m\t\n",
      "Epoch    48 | LossA: \u001b[32m0.13(-4.47%) ▼\u001b[0m\t| LossAB: \u001b[91m2.99(+57.13%) ▲\u001b[0m\t\n",
      "30/30 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.5666666626930237\n"
     ]
    }
   ],
   "source": [
    "y_test_oh = dense_to_one_hot(y_test, num_clases=4)\n",
    "\n",
    "# visualizing losses and accuracy\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "#Observing the losses but can be commented out as it's not mandatory \n",
    "reporter = lossprettifier.LossPrettifier(show_percentage=True)\n",
    "\n",
    "for i in range(numEpochs-1):\n",
    "    reporter(epoch=i, LossA = train_loss[i], LossAB = val_loss[i])\n",
    "\n",
    "# Model evaluation \n",
    "score, acc = model.evaluate(x_test, y_test_oh, batch_size=batch_Size)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "#if acc>0.675:\n",
    "model.save_weights(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67        10\n",
      "           1       0.57      0.80      0.67        10\n",
      "           2       0.38      0.30      0.33        10\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.57      0.57      0.56        30\n",
      "weighted avg       0.57      0.57      0.56        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.reshape(len(y_test), 4)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Writing results on file\n",
    "f = open(resultPath,'a') #create classification report\n",
    "f.write(classification_report(y_test, y_pred))\n",
    "f.write(str(sklm.cohen_kappa_score(y_test, y_pred))+\",\"+str(acc)+\",\"+str(score)+\"\\n\")\n",
    "\n",
    "#Print class-wise classification metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wbvenv36",
   "language": "python",
   "name": "wbvenv36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
